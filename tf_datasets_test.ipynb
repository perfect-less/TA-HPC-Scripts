{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 02:46:38.681511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 02:46:38.725588: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-05-10 02:46:38.725612: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-05-10 02:46:38.726135: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.api import keras\n",
    "from tensorflow.keras.utils import timeseries_dataset_from_array\n",
    "\n",
    "from hpcscripts.sharedutils.trainingutils import LoadModel, SetLowTFVerbose, MakeSinglePrediction\n",
    "from hpcscripts.sharedutils.nomalization import DF_Nomalize, denorm\n",
    "from hpcscripts.sharedutils.modelutils import SelectModelPrompt\n",
    "from hpcscripts.option import pathhandler as ph\n",
    "from hpcscripts.option import globalparams as G_PARAMS\n",
    "\n",
    "SetLowTFVerbose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train count: 350\n",
      "Test count : 61\n"
     ]
    }
   ],
   "source": [
    "train_dir = ph.GetProcessedPath(\"Selected\")\n",
    "test_dir  = ph.GetProcessedPath(\"Test\")\n",
    "\n",
    "train_list = os.listdir(train_dir)\n",
    "test_list  = os.listdir(test_dir)\n",
    "\n",
    "for i, test_file in enumerate (test_list):\n",
    "    train_list.remove(test_file)\n",
    "    test_list[i] = os.path.join(test_dir, test_file)\n",
    "\n",
    "for i, train_file in enumerate (train_list):\n",
    "    train_list[i] = os.path.join(train_dir, train_file)\n",
    "\n",
    "print (\"Train count: {}\".format(len(train_list)))\n",
    "print (\"Test count : {}\".format(len(test_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train count: 350\n",
      "Test count : 3\n"
     ]
    }
   ],
   "source": [
    "# train_list = train_list[:3]\n",
    "test_list  = test_list[:3]\n",
    "\n",
    "print (\"Train count: {}\".format(len(train_list)))\n",
    "print (\"Test count : {}\".format(len(test_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Version\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def to_datasets(files: List[str]):\n",
    "\n",
    "    np_data = []\n",
    "\n",
    "    for i, file in enumerate(files):\n",
    "        try:\n",
    "            data = pd.read_csv(file)\n",
    "            data = data.loc[:, \n",
    "                            [\"hralt_m\", \"theta_rad\", \"aoac_rad\", \"cas_mps\", \"elv_l_rad\"]\n",
    "                    ]\n",
    "        except:\n",
    "            raise Exception(\"Can't process {}\".format(file))\n",
    "\n",
    "        ds = timeseries_dataset_from_array(\n",
    "                    data.to_numpy(),\n",
    "                    targets=None,\n",
    "                    sequence_length=5,\n",
    "                    sequence_stride=1,\n",
    "                    batch_size=99999\n",
    "                )\n",
    "        \n",
    "        for elem in ds.take(1):\n",
    "            np_data.append(\n",
    "                    elem.numpy()\n",
    "                )\n",
    "            print (\"{}, i: {}\".format(elem.numpy().shape, i), end='\\r')\n",
    "    \n",
    "    print()\n",
    "    concated_np = np.concatenate(np_data)\n",
    "    return concated_np\n",
    "    # return tf.data.Dataset.from_tensor_slices(concated_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(397, 5, 5), i: 349\n",
      "108904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorSpec(shape=(32, 5, 5), dtype=tf.float64, name=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = to_datasets(train_list)\n",
    "\n",
    "# for elem in train_ds:\n",
    "#     print (elem)\n",
    "\n",
    "shuffle_size = train_ds.shape[0]\n",
    "print (shuffle_size)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(train_ds).shuffle(shuffle_size).batch(32, drop_remainder=True)\n",
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 5, 5), Count: 3403\n",
      "3403                                                                                                \n",
      "Total: 108896\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for elem in train_ds:\n",
    "    count += 1\n",
    "    print (\"{}, Count: {}\".format(elem.shape, count), end='\\r')\n",
    "\n",
    "print ()\n",
    "print (str(count).ljust(100, \" \"))\n",
    "print (\"Total: {:.0f}\".format(count*32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With WindowGenerator Class version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition for WindowGenerator are written based on tensorflow official\n",
    "# tutorial on:\n",
    "#        https://www.tensorflow.org/tutorials/structured_data/time_series\n",
    "# which licensed under Apache License, Version 2.0\n",
    "\n",
    "\n",
    "\n",
    "# Copyright 2019 The TensorFlow Authors\n",
    "\n",
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from typing import List\n",
    "\n",
    "class WindowGenerator():\n",
    "\n",
    "    USABLE_COLUMNS  = set (G_PARAMS.FEATURE_COLUMNS + G_PARAMS.SEQUENTIAL_LABELS)\n",
    "    FEATURE_COLUMNS = G_PARAMS.FEATURE_COLUMNS\n",
    "\n",
    "    def __init__(self, input_width:int, label_width:int=1, shift:int=1,\n",
    "                train_list=None, test_list=None, val_list=None,\n",
    "                label_columns=None,\n",
    "                shuffle_train:bool=True):\n",
    "        # Store list of the data.\n",
    "        self.train_list = train_list\n",
    "        self.test_list = test_list\n",
    "        self.val_list = val_list\n",
    "\n",
    "        # Set shuffle_train\n",
    "        self.shuffle_train = shuffle_train\n",
    "\n",
    "        # Work out the label column indices.\n",
    "        self.input_columns = self.FEATURE_COLUMNS\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in enumerate(label_columns)}\n",
    "\n",
    "        self.column_indices = self.__get_column_indices()\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "    def __get_column_indices(self):\n",
    "        # Get 1 Sample DataFrame and use it to determine column indices\n",
    "        df = pd.read_csv(self.train_list[0])\n",
    "        return {name: i for i, name in enumerate(df.columns) if name in self.USABLE_COLUMNS}\n",
    "\n",
    "    def split_window(self, features):\n",
    "        inputs = features[:, self.input_slice, :]\n",
    "        labels = features[:, self.labels_slice, :]\n",
    "        if self.input_columns is not None:\n",
    "            inputs = tf.stack(\n",
    "                            [inputs[:, :, self.column_indices[name]] for name in self.input_columns],\n",
    "                            axis=-1\n",
    "                    )\n",
    "        if self.label_columns is not None:\n",
    "            labels = tf.stack(\n",
    "                            [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "                            axis=-1\n",
    "                    )\n",
    "\n",
    "        # Slicing doesn't preserve static shape information, so set the shapes\n",
    "        # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "        return inputs, labels\n",
    "    \n",
    "    def make_dataset(self, file_list: List[str], do_shuffle:bool = False):\n",
    "        np_data = []\n",
    "        for file in file_list:\n",
    "            try:\n",
    "                data = pd.read_csv(file)\n",
    "                #\n",
    "                # PREPROCESS HERE, \n",
    "                # NORMALIZATION, ETC.\n",
    "            except:\n",
    "                raise Exception(\"Can't process {}\".format(file))\n",
    "\n",
    "            ds = timeseries_dataset_from_array(\n",
    "                        data=data.to_numpy(),\n",
    "                        targets=None,\n",
    "                        sequence_length=self.total_window_size,\n",
    "                        sequence_stride=1,\n",
    "                        batch_size=99999\n",
    "                    )\n",
    "            \n",
    "            for elem in ds.take(1):\n",
    "                np_data.append(\n",
    "                        elem.numpy()\n",
    "                    )\n",
    "        \n",
    "        concated_np = np.concatenate(np_data)\n",
    "        buffer_size = concated_np.shape[0]\n",
    "\n",
    "        if do_shuffle:\n",
    "            ds = tf.data.Dataset.from_tensor_slices(concated_np).shuffle(buffer_size).batch(32, drop_remainder=True)\n",
    "        else:\n",
    "            ds = tf.data.Dataset.from_tensor_slices(concated_np).batch(32, drop_remainder=True)\n",
    "        ds = ds.map(self.split_window)\n",
    "\n",
    "        return ds\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'])\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        return self.make_dataset(self.train_list, self.shuffle_train)\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        return self.make_dataset(self.val_list)\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        return self.make_dataset(self.test_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Total window size: 1\n",
       "Input indices: [0]\n",
       "Label indices: [0]\n",
       "Label column name(s): ['elv_l_rad', 'theta_rad']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = WindowGenerator(\n",
    "                    input_width=1, shift=0,\n",
    "                    train_list=train_list,\n",
    "                    label_columns=[\"elv_l_rad\", \"theta_rad\"],\n",
    "                    shuffle_train=False\n",
    "                )\n",
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = w1.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape (batch, time, features): (32, 1, 4)\n",
      "Labels shape (batch, time, features): (32, 1, 2)\n",
      "tf.Tensor([[ 5.51992800e+02  1.15045175e-03 -3.29803196e-02  8.74548000e+01]], shape=(1, 4), dtype=float64)\n",
      "tf.Tensor([[-0.07680304  0.00115045]], shape=(1, 2), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "for example_inputs, example_labels in train_ds.take(1):\n",
    "    print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n",
    "    print(f'Labels shape (batch, time, features): {example_labels.shape}')\n",
    "\n",
    "    print (example_inputs[0, :, :])\n",
    "    print (example_labels[0, :, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>theta_rad</th>\n",
       "      <th>cas_mps</th>\n",
       "      <th>aoac_rad</th>\n",
       "      <th>hralt_m</th>\n",
       "      <th>elv_l_rad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001150</td>\n",
       "      <td>87.454800</td>\n",
       "      <td>-0.032980</td>\n",
       "      <td>551.9928</td>\n",
       "      <td>-0.076803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001342</td>\n",
       "      <td>87.390495</td>\n",
       "      <td>-0.038349</td>\n",
       "      <td>552.6024</td>\n",
       "      <td>-0.077874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.003451</td>\n",
       "      <td>87.229732</td>\n",
       "      <td>-0.037582</td>\n",
       "      <td>552.2976</td>\n",
       "      <td>-0.075732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.003835</td>\n",
       "      <td>87.326190</td>\n",
       "      <td>-0.042184</td>\n",
       "      <td>546.2016</td>\n",
       "      <td>-0.073589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.003643</td>\n",
       "      <td>87.390495</td>\n",
       "      <td>-0.029145</td>\n",
       "      <td>546.5064</td>\n",
       "      <td>-0.072518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   theta_rad    cas_mps  aoac_rad   hralt_m  elv_l_rad\n",
       "0   0.001150  87.454800 -0.032980  551.9928  -0.076803\n",
       "1  -0.001342  87.390495 -0.038349  552.6024  -0.077874\n",
       "2  -0.003451  87.229732 -0.037582  552.2976  -0.075732\n",
       "3  -0.003835  87.326190 -0.042184  546.2016  -0.073589\n",
       "4  -0.003643  87.390495 -0.029145  546.5064  -0.072518"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_df = pd.read_csv(train_list[0])\n",
    "_df = _df.loc[:, w1.USABLE_COLUMNS]\n",
    "_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4680a687c0b493607a5a84d99aa6299372553ac8fbb569a372de15fbb66a08e8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
